working_dir: ./
excludes: ["/.git/"]
env_vars:
  TOKENIZERS_PARALLELISM: "true"
  NCCL_DEBUG: "WARN"
  VLLM_LOGGING_LEVEL: "WARN"
  TORCH_NCCL_AVOID_RECORD_STREAMS: "1"
  PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:False"
  CUDA_DEVICE_MAX_CONNECTIONS: "1"
  VLLM_ALLREDUCE_USE_SYMM_MEM: "0"
  RAY_verbose_spill_logs: "1"
  RAY_LOG_TO_STDERR: "1"
  RAY_BACKEND_LOG_LEVEL: "debug"
  RAY_memstore_allow_fallback_to_memory: "0"
  # Torch shared memory strategy to file system reduces /dev/shm usage
  TORCH_SHARE_STRATEGY: "file_system"
